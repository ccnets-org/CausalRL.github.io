<!DOCTYPE html>
<html>
    <head>
        <meta http-equiv="Content-type" content="text/html;charset=UTF-8">
        <link rel="stylesheet" href="style.css">
        <title>API</title>
    </head>
    <body class="vscode-body vscode-light">
        <div data-line="0" class="code-line" dir="auto"></div>
<nav class="navbar">
<div data-line="2" class="code-line" dir="auto"></div>
  <a class="header-link" href="index.html">  
    <img src="https://github.com/ccnets-org/CausalRL.github.io/assets/95277008/6db8e929-637b-4d9c-9a61-20910e878d53" alt="CCNets Logo" class="logo">
    <!-- <h1 class="page-name">CausalRL Documentation</h1> -->
  </a>
<div data-line="5" class="code-line" dir="auto"></div>
<div class="social-links">
  <a href="https://ccnets.org/" target="_blank">
    <img src="https://github.com/ccnets-team/rl-tune/assets/95277008/42f10f53-0262-4203-9c6f-618e4841adb2" alt="Website" style="width: 20px; height: 20px;">
  </a>
  <a href="https://github.com/ccnets-team/rl-tune" target="_blank">
    <img src="https://github.com/ccnets-team/rl-tune/assets/95277008/5183f887-1263-408b-8c2f-9eeefafbce48" alt="GitHub" style="width: 20px; height: 20px;">
  </a>
  <a href="https://www.linkedin.com/company/ccnets/" target="_blank">
    <img src="https://github.com/ccnets-team/rl-tune/assets/95277008/fe1f76f2-a407-4f58-ad28-92a9ac9efa3f" alt="LinkedIn" style="width: 20px; height: 20px;">
  </a>
</div>
</nav>
<div data-line="7" class="code-line" dir="auto"></div>
<div class="sidebar">
  <ul>
  <li>
    <a href="index.html">Introduction</a>
    <ul>
      <li><a href="index.html#basic-usage">Basic Usage</a></li>
    </ul>
  </li>
  <li>
    <a href="API.html">API</a>
    <ul>
      <li><a href="API.html#class-rltune">RLTune</a></li>
      <li><a href="API.html#class-causalrl">CausalRL</a></li>
      <li><a href="API.html#class-rl-params">RLParams</a></li>
    </ul>
  </li>
  <li>
    <a href="Env Wrapper.html">Env Wrapper</a>
    <ul>
      <li><a href="Env Wrapper.html#gym-wrapper">Gym Wrapper</a></li>
      <li><a href="#unity-mlagent-wrapper">Unity MLAgent Wrapper</a></li>
    </ul>
  </li>
  <li>
    <a href="#">Tutorials</a>
    <ul>
      <li><a href="./tutorials/Tutorial_Day1.html">Day 1</a></li>
      <li><a href="#">Day 2</a></li>
    </ul>
  </li>
  <li>
    <a href="#">Development</a>
    <ul>
      <li><a href="https://github.com/ccnets-team/rl-tune">Github</a></li>
      <li><a href="https://wandb.ai/rl_tune/rl-tune-gym/?workspace">W&amp;B</a></li>
    </ul>
  </li>
</ul>
</div>
<div data-line="61" class="code-line" dir="auto"></div>
<div class="main-content">
<h1 data-line="63" class="code-line" dir="auto" id="api" tabindex="-1">API</h1>
<h2 data-line="65" class="code-line" dir="auto" id="class-rltune" tabindex="-1">Class RLTune <a href="https://github.com/ccnets-team/rl-tune/blob/beta-causal-rl/rl_tune.py" target="_blank" style="font-size: smaller;">[source]</a></h2>
<pre><code data-line="66" class="code-line language-python" dir="auto"><span class="hljs-keyword">class</span> <span class="hljs-title class_">RLTune</span>:
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, env_config: EnvConfig, rl_params: RLParameters, device, use_graphics=<span class="hljs-literal">False</span>, use_print=<span class="hljs-literal">False</span>, use_wandb = <span class="hljs-literal">False</span></span>):
</code></pre>
<p data-line="70" class="code-line" dir="auto">The RLTune class is designed for tuning and training reinforcement learning models, using a specified trainer.</p>
<h3 data-line="72" class="code-line" dir="auto" id="parameters" tabindex="-1">Parameters:</h3>
<ul data-line="73" class="code-line" dir="auto">
<li data-line="73" class="code-line" dir="auto"><code>env_config</code> (EnvConfig): Configuration for the environment.</li>
<li data-line="74" class="code-line" dir="auto"><code>trainer</code> (Trainer): Trainer object for the reinforcement learning algorithm.</li>
<li data-line="75" class="code-line" dir="auto"><code>device</code> (Device): Computational device (e.g., CPU, GPU).</li>
<li data-line="76" class="code-line" dir="auto"><code>use_graphics</code> (bool, optional): Whether to use graphics during training/testing. Default is False.</li>
<li data-line="77" class="code-line" dir="auto"><code>use_print</code> (bool, optional): Whether to print training/testing logs. Default is False.</li>
</ul>
<h3 data-line="79" class="code-line" dir="auto" id="functions" tabindex="-1">Method Overview</h3>
<ul data-line="81" class="code-line" dir="auto">
<li data-line="81" class="code-line" dir="auto">
<p data-line="81" class="code-line" dir="auto"><code>__init__():</code>
Accepts environment configuration (EnvConfig), RL parameters (RLParameters), computational device (device), and options for graphics, printing, and WandB integration.</p>
</li>
<li data-line="84" class="code-line" dir="auto">
<p data-line="84" class="code-line" dir="auto"><code>_end_environments():</code>
If the environment exists, shut down both the training and testing environments of a reinforcement learning setup.</p>
</li>
<li data-line="87" class="code-line" dir="auto">
<p data-line="87" class="code-line" dir="auto"><code>train():</code>
The main method of this class. To train the model based on the provided policy.</p>
<ul data-line="90" class="code-line" dir="auto">
<li data-line="90" class="code-line" dir="auto">setup(training=True): To prepare the training environment using this method</li>
</ul>
</li>
<li data-line="92" class="code-line" dir="auto">
<p data-line="92" class="code-line" dir="auto"><code>test():</code>
To evaluate a reinforcement learning algorithm over specified of episodes(default: max_episode=100)</p>
<ul data-line="94" class="code-line" dir="auto">
<li data-line="94" class="code-line" dir="auto">setup(training=False): To prepare the test environment using this method</li>
</ul>
</li>
<li data-line="96" class="code-line" dir="auto">
<p data-line="96" class="code-line" dir="auto"><code>_train_step_logic():</code>
The core method of the training process in a reinforcement learning environment.</p>
<ul data-line="99" class="code-line" dir="auto">
<li data-line="99" class="code-line" dir="auto"><code>helper.init_step():</code> initializes the training/testing step</li>
<li data-line="100" class="code-line" dir="auto"><code>process_test_environmemt():</code> to handle interactions with an environment and for test interactions(<code>if 'training' == False</code>), the trajectories are not pushed to memory, which is a step reserved for training interaction.</li>
</ul>
</li>
<li data-line="102" class="code-line" dir="auto">
<p data-line="102" class="code-line" dir="auto"><code>_test_step_logic():</code>
The logic for each step during the testing phase of a reinforcement learning algorithm.</p>
</li>
<li data-line="105" class="code-line" dir="auto">
<p data-line="105" class="code-line" dir="auto"><code>train_on_policy():</code>
To train a model using on-policy reinforcement learning algorithm.</p>
<ul data-line="108" class="code-line" dir="auto">
<li data-line="108" class="code-line" dir="auto"><code>process_train_environment():</code> Interacts with the training environment and collect data based on the current policy.</li>
<li data-line="109" class="code-line" dir="auto"><code>should_update_strategy():</code> check if strategy should be updated.</li>
<li data-line="110" class="code-line" dir="auto"><code>_update_strategy_from_samples():</code> to enhance the strategy used in the training of a reinforcement learning.</li>
<li data-line="111" class="code-line" dir="auto"><code>reset_memory_and_train():</code> reset the memory buffer and then performs a seies of training steps. It iterates over a predefined number of on-policy iterations(<code>train_step</code>)</li>
</ul>
</li>
<li data-line="113" class="code-line" dir="auto">
<p data-line="113" class="code-line" dir="auto"><code>train_off_policy():</code>
To train a model using off-policy reinforcement learning alrorithm.</p>
<ul data-line="116" class="code-line" dir="auto">
<li data-line="116" class="code-line" dir="auto"><code>process_train_environment():</code> Interacts with the training environment and collect data based on the current policy.</li>
<li data-line="117" class="code-line" dir="auto"><code>should_update_strategy():</code> check if strategy should be updated.</li>
<li data-line="118" class="code-line" dir="auto"><code>_update_strategy_from_samples():</code> to enhance the strategy used in the training of a reinforcement learning.</li>
<li data-line="119" class="code-line" dir="auto"><code>train_step():</code> a single step of training in a reinforcement learning include Sample Trajectory, Model Training, TD Errors and Record Metrics</li>
</ul>
</li>
<li data-line="122" class="code-line" dir="auto">
<p data-line="122" class="code-line" dir="auto"><code>interact_environment():</code> This function progresses the environmentstate, retrieves the resulting trajectories, and the records these trajectories using the helper.record method.</p>
</li>
</ul>
<br>
<br>
<h2 data-line="125" class="code-line" dir="auto" id="class-causalrl" tabindex="-1">Class CausalRL <a href="https://github.com/ccnets-team/rl-tune/blob/beta-causal-rl/training/causal_rl.py" target="_blank" style="font-size: smaller;">[source]</a></h2>
<pre><code data-line="126" class="code-line language-python" dir="auto"><span class="hljs-keyword">class</span> <span class="hljs-title class_">CausalRL</span>(<span class="hljs-title class_ inherited__">BaseTrainer</span>):
	<span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, env_config, rl_params, device</span>):
</code></pre>
<p data-line="131" class="code-line" dir="auto">This class is specialized trainer for reinforcement learning. It initailizes with environment configuration('env_config'), RLparameters('rl_params'), and the computational device('device').
  And the class sets up three key networks: Critic, Actor, Reverse-Environment networks, each configured based on the provided RL parameters.</p>
  <h3 data-line="134" class="code-line" dir="auto" id="funtions" tabindex="-1">Method Overview</h3>
  <ul data-line="135" class="code-line" dir="auto">
    <li data-line="135" class="code-line" dir="auto">
      <p data-line="135" class="code-line" dir="auto"><code>get_action()</code> method is responsible for determining the action to be taken by an agent in a given state. When <code>training == True</code>, it uses Actor's <code>sample_action()</code> method else, it uses <code>select_action()</code> method.</p>
    </li>
    <li data-line="137" class="code-line" dir="auto">
      <p data-line="137" class="code-line" dir="auto"><code>train_model()</code> method is a core method of CausalRL approach. It involves a cooperative setup among three networks: Critic, Actor, Reverse-Environment, which learn from the environment's transitions. These method include computing various costs, errors, losses and these losses are used for backpropagation to adjust network parameters.</p>
    </li>
    <li data-line="139" class="code-line" dir="auto">
      <p data-line="139" class="code-line" dir="auto"><code>backwards()</code> method  conducts backpropagation for multiple neural networks in a reinforcement learning framework, each targeting a specific part of the causal relationship graph. It first disables, then selectively enables gradients for each network during the error backpropagation process to ensure targeted learning. Finally, it resets the networks to allow gradient updates, preparing them for future forward passes.</p>
    </li>
  </ul>
<br>
<br>
  <h2 data-line="142" class="code-line" dir="auto" id="class-rl-params" tabindex="-1">Class RLParams <a href="https://github.com/ccnets-team/rl-tune/blob/beta-causal-rl/utils/setting/rl_params.py" target="_blank" style="font-size: smaller;">[source]</a></h2>
  <p data-line="70" class="code-line" dir="auto">The <code>RLParameters</code> class is designed as a structured approach to manage the settings for Reinforcement Learning (RL) configurations. This class encapsulates and manages multiple subsidiary parameter classes, each representing different components of an RL system. The detailed parameters for each component within the <code>RLParams</code> can be found in the <code>rl_config.py</code> </p>
  <h3 data-line="144" class="code-line" dir="auto" id="class-trainingparameters" tabindex="-1">Class TrainingParameters</h3>
  <ul data-line="145" class="code-line" dir="auto">
    <!-- <li data-line="145" class="code-line" dir="auto"><code>trainer_name</code>: Specifies the type of trainer algorithm. (default: <code>causal_rl</code>)</li> -->
    <!-- <li data-line="146" class="code-line" dir="auto"><code>trainer_variant</code>: Specifies a variant of <code>CausalRL</code>. (default: <code>classic</code>)</li> -->
    <!-- <li data-line="147" class="code-line" dir="auto"><code>use_on_policy</code>: Set whether the training is <code>on-policy</code> or <code>off-policy</code>. (default: <code>False</code>)</li> -->
    <li data-line="148" class="code-line" dir="auto"><code>batch_size</code>: Number of samples processed before model update. Larger batch szie can lead to more stable but slower training. (batch size / samples per step, default: <code>64</code>)</li>
    <li data-line="149" class="code-line" dir="auto"><code>replay_ratio</code>: Ratio foe how often past experiences are reused. (default: <code>1</code>)</li>
    <li data-line="150" class="code-line" dir="auto"><code>train_interval</code>: Frequency of thraining updates. (default: <code>1</code>)</li>
  </ul>
  <h3 data-line="153" class="code-line" dir="auto" id="class-algorithmparameters" tabindex="-1">Class AlgorithmParameters</h3>
  <ul data-line="154" class="code-line" dir="auto">
    <!-- <li data-line="154" class="code-line" dir="auto"><code>min_seq_length</code>: Minimum sequence length during exploration. (default: <code>1</code>)</li> -->
    <li data-line="155" class="code-line" dir="auto"><code>gpt_seq_length</code>: Maximum sequence length for training and exploration. (default: <code>16</code>)</li>
<li data-line="156" class="code-line" dir="auto"><code>discount_factor</code>: To reduce the value of future rewards in the calculation of expected returns. (default: <code>0.99</code>)</li>
<li data-line="157" class="code-line" dir="auto"><code>advantage_lambda</code>: Weighting advantages in policy optimization. (default: <code>0.99</code>)</li>
<!-- <li data-line="158" class="code-line" dir="auto"><code>use_gae_advantage</code>: Automatically set in <code>rl_trainer</code>.py. It's determined based on weather the policy is off-policy or on-policy. (default: <code>None</code>)</li> -->
</ul>
<h3 data-line="161" class="code-line" dir="auto" id="class-networkparameters" tabindex="-1">Class NetworkParameters</h3>
<ul data-line="162" class="code-line" dir="auto">
<li data-line="163" class="code-line" dir="auto"><code>num_layer</code>: Defines the number of layers in the neural network. (default: <code>5</code>)</li>
<li data-line="164" class="code-line" dir="auto"><code>d_model</code>: Specifies the dimensionality of the input and output of the model layers. (default: <code>256</code>)</li>
<li data-line="165" class="code-line" dir="auto"><code>dropout</code>: Sets the dropout rate to prevent overfitting. This is a regularization technique where randomly selected neurons are ignored during training. (default: <code>0.01</code>)</li>
<li data-line="164" class="code-line" dir="auto"><code>network_type</code>:  Determines the type of neural network architecture to use. This parameter allows the selection of different network architectures if needed. (default: <code>GPT</code>)</li>
<li data-line="165" class="code-line" dir="auto"><code>critic_params</code>: Parameters for the critic network.</li>
<li data-line="166" class="code-line" dir="auto"><code>actor_params</code>: Parameters for the actor network.</li>
<li data-line="167" class="code-line" dir="auto"><code>rev_env_params</code>: Parameters for the rev-env network.</li>
</ul>
<h3 data-line="170" class="code-line" dir="auto" id="class-optimizationparameters" tabindex="-1">Class OptimizationParameters</h3>
<ul data-line="171" class="code-line" dir="auto">
<li data-line="171" class="code-line" dir="auto"><code>lr</code>: Learning rate for the optimiztion algorithm. (default: <code>2e-5</code>)</li>
<li data-line="172" class="code-line" dir="auto"><code>min_lr</code>: Minimum learning rate to which the lr will decay. (default: <code>5e-1</code>)</li>
<li data-line="173" class="code-line" dir="auto"><code>scheduler_type</code>: Specifies the type of learning rate scheduler. (default: <code>exponential</code>)</li>
<li data-line="174" class="code-line" dir="auto"><code>tau</code>: How quickly the target network is updated with the main network's weights. (default: <code>1e-1</code>)</li>
<li data-line="175" class="code-line" dir="auto"><code>use_target_network</code>: Indicate whether to use target network or not. (default: <code>True</code>)</li>
<li data-line="176" class="code-line" dir="auto"><code>clip_grad_range</code>: The range within which gradients are clipped. To prevent the issue of exploding gradients. (default: <code>None</code>)</li>
</ul>
<h3 data-line="179" class="code-line" dir="auto" id="class-explorationparameters" tabindex="-1">Class ExplorationParameters</h3>
<ul data-line="180" class="code-line" dir="auto">
<li data-line="180" class="code-line" dir="auto"><code>noise_type</code>: Type of exploration noise used to encourage exploration in the agent. (default: <code>None</code>)</li>
<li data-line="181" class="code-line" dir="auto"><code>max_steps</code>: Maximum number of steps for the exploration phase. (default: <code>100000</code>)</li>
</ul>
<h3 data-line="184" class="code-line" dir="auto" id="class-memoryparameters" tabindex="-1">Class MemoryParameters</h3>
<ul data-line="185" class="code-line" dir="auto">
<li data-line="185" class="code-line" dir="auto"><code>buffer_size</code>: Total size of the memory buffer, This determines the capacity for storing past experiences. (default: <code>256000</code>)</li>
<!-- <li data-line="186" class="code-line" dir="auto"><code>early_training_start_step</code>: An optional parameter that allows training to commence before the replay buffer is completely filled. (default: <code>None</code>)</li> -->
</ul>
<h3 data-line="189" class="code-line" dir="auto" id="class-normalizationparameters" tabindex="-1">Class NormalizationParameters</h3>
<ul data-line="190" class="code-line" dir="auto">
<li data-line="190" class="code-line" dir="auto">
<p data-line="190" class="code-line" dir="auto"><code>state_normalizer</code>: Determines the method used for normalizing state valeus. (default: <code>running_mean_std</code>)</p>
</li>
<li data-line="191" class="code-line" dir="auto">
<p data-line="191" class="code-line" dir="auto"><code>reward_normalizer</code>: Determines the method used for normalizing reward values. (default: <code>running_mean_std</code>)</p>
</li>
<li data-line="192" class="code-line" dir="auto">
<p data-line="192" class="code-line" dir="auto"><code>advantage_normalizer</code>: Determines the method used for normalizing advantage values. (default: <code>L1_norm</code>)</p>
</li>
<!-- <li data-line="193" class="code-line" dir="auto">
<p data-line="193" class="code-line" dir="auto"><code>exponential_moving_alpha</code>: Set the alpha value for exponential moving average calculations. (dafault: <code>1e-4</code>)</p>
</li> -->
<!-- <li data-line="194" class="code-line" dir="auto">
<p data-line="194" class="code-line" dir="auto"><code>clip_norm_range</code>: The range within which normalized values are clipped. (default: <code>10.0</code>)</p>
</li> -->
<div data-line="196" class="code-line" dir="auto"></div>
<div align="left" style="padding-bottom: 10px;">
  Copyright © 2024 CCNets
</div>
<div data-line="200" class="code-line" dir="auto"></div>

</ul>
<div data-line="213" class="code-line" dir="auto"></div>
</div>
<div data-line="215" class="code-line" dir="auto"></div>

       <script src="script.js"></script>
    </body>
</html>
