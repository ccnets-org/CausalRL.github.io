<!DOCTYPE html>
<html>
    <head>
      <!-- Google tag (gtag.js) -->
      <script async src="https://www.googletagmanager.com/gtag/js?id=G-9PP6S33V9K"></script>
      <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-9PP6S33V9K');
      </script>
        <meta http-equiv="Content-type" content="text/html;charset=UTF-8">
        <title>CausalRL Documentation | API</title>

        <link rel="stylesheet" href="./style/style.css">
        <link rel="icon" href="https://github.com/ccnets-org/CausalRL.github.io/assets/95277008/2573d62b-0ea3-4ca7-ab78-df785b6f51f3" type="image/png">
        <link rel="shortcut icon" href="https://github.com/ccnets-org/CausalRL.github.io/assets/95277008/2573d62b-0ea3-4ca7-ab78-df785b6f51f3" type="image/png">
    </head>
    <body class="vscode-body vscode-light">
        <div data-line="0" class="code-line" dir="auto"></div>
<nav class="navbar">
<div data-line="2" class="code-line" dir="auto"></div>
  <a class="header-link" href="index.html">  
    <img src="https://github.com/ccnets-org/CausalRL.github.io/assets/95277008/6db8e929-637b-4d9c-9a61-20910e878d53" alt="CCNets Logo" class="logo">
    <!-- <h1 class="page-name">CausalRL Documentation</h1> -->
  </a>
<div data-line="5" class="code-line" dir="auto"></div>
<div class="social-links">
  <a href="https://ccnets.org/" target="_blank">
    <img src="https://github.com/ccnets-team/rl-tune/assets/95277008/42f10f53-0262-4203-9c6f-618e4841adb2" alt="Website" style="width: 20px; height: 20px;">
  </a>
  <a href="https://github.com/ccnets-team/rl-tune" target="_blank">
    <img src="https://github.com/ccnets-team/rl-tune/assets/95277008/5183f887-1263-408b-8c2f-9eeefafbce48" alt="GitHub" style="width: 20px; height: 20px;">
  </a>
  <a href="https://www.linkedin.com/company/ccnets/" target="_blank">
    <img src="https://github.com/ccnets-team/rl-tune/assets/95277008/fe1f76f2-a407-4f58-ad28-92a9ac9efa3f" alt="LinkedIn" style="width: 20px; height: 20px;">
  </a>
</div>
</nav>
<div data-line="7" class="code-line" dir="auto"></div>
<div class="sidebar">
  <ul>
  <li>
    <a href="index.html">INTRODUCTION</a>
    <ul>
      <li><a href="index.html#basic-usage">Basic Usage</a></li>
    </ul>
  </li>
  <li>
    <a href="API.html">API</a>
    <ul>
      <li><a href="API.html#class-causalrl">CausalRL</a></li>
      <li><a href="API.html#class-rl-config">RLParameters</a></li>
      <li><a href="API.html#class-causaltrainer">CausalTrainer</a></li>
    </ul>
  </li>
  <li>
    <a>ENVIRONMENTS</a>
    <ul>
    <li><a href="analyze_env.html">Analyze_env</a></li>
    <li><a href="Env Wrapper.html#gym-wrapper">OpenAI Gymnasium</a></li>
    <li><a href="Env Wrapper.html#unity-mlagent-wrapper">Unity MLAgents</a></li>
    </ul>
</li>
<li>
  <a>TUTORIAL</a>
  <ul>
      <li class="dropdown-menu">
          <a class="dropbtn" href="#">Basic <span class="dropdown-arrow">&#9660;</span></a>
          <ul class="dropdown-content">
              <li><a href="./tutorials/Tutorial_supernet_EN.html">Unlocking CausalRL</a></li>
              <!-- <li><a href="#">Day 2</a></li> -->
          </ul>
      </li>
  </ul>
</li>
  <li>
    <a>DEVELOPMENT</a>
    <ul>
        <li>
            <a href="https://github.com/ccnets-team/rl-tune" target="_blank">
            Github    
            <img src="https://github.com/ccnets-org/CausalRL.github.io/assets/95277008/9f93117b-8082-4b92-ac77-245b2e13eeba" alt="Github" style="height:10px; width:10px;">
            </a>
        </li>
        <li><a href="https://wandb.ai/rl_tune/rl-tune-gym/?workspace" target="_blank">
            W&amp;B
            <img src="https://github.com/ccnets-org/CausalRL.github.io/assets/95277008/9f93117b-8082-4b92-ac77-245b2e13eeba" alt="W&B" style="height:10px; width:10px;">
            </a>
        </li>
        <li><a href="./release_note.html">
          CausalRL Release Notes
          </a>
      </li>
    </ul>
  </li>
</ul>
</div>
<div data-line="61" class="code-line" dir="auto"></div>
<div class="toc">
  <h3>INDEX</h3>
  <ul id="toc-list"></ul>
</div>
<div class="main-content" id="main-content">
  <h1 data-line="63" class="page-title" dir="auto" id="api" tabindex="-1">API</h1>
  <div class="inline-container">
    <h2 data-line="91" class="class-causalrl" dir="auto" id="class-causalrl" tabindex="-1">Class CausalRL</h2>
    <a href="https://github.com/ccnets-team/rl-tune/blob/beta-causal-rl/causal_rl.py" target="_blank" style="font-weight: 600; margin-left: 10px; margin-bottom:10px; font-size: 10px; font-size: smaller;">[source]</a>
  </div>
  <pre><code data-line="66" class="code-line language-python" dir="auto"><span class="hljs-keyword">class</span> <span class="hljs-title class_">CausalRL</span>:
      <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, rl_params: RLParameters, device, use_print=<span class="hljs-literal">False</span>, use_wandb = <span class="hljs-literal">False</span></span>):
  </code></pre>
  <p data-line="70" class="code-line" dir="auto">The CausalRL class is designed for tuning and training reinforcement learning models, using a specified trainer.</p>
  <h3 data-line="72" class="code-line" dir="auto" id="parameters" tabindex="-1">Parameters:</h3>
  <ul data-line="73" class="code-line" dir="auto">
  <!-- <li data-line="73" class="code-line" dir="auto"><code>env_config</code> (EnvConfig): Configuration for the environment.</li> -->
  <li data-line="74" class="code-line" dir="auto"><code>rl_params</code> (RLParameters): Defines a structure for managing various types of parameters used in reinforcement learning (RL) settings, including training, algorithm, network, optimization, and normalization configurations.</li>
  <li data-line="75" class="code-line" dir="auto"><code>device</code> (Device): Computational device (e.g. CPU, GPU).</li>
  <li data-line="76" class="code-line" dir="auto"><code>use_print</code> (bool, optional): Whether to print training/testing logs. Default is False.</li>
  <li data-line="77" class="code-line" dir="auto"><code>use_wandb</code> (bool, optional): Whether to record log to W&B during training/testing. Default is False.</li>
  </ul>
  <h3 data-line="79" class="code-line" dir="auto" id="functions" tabindex="-1">Method Overview</h3>
  <ul data-line="81" class="code-line" dir="auto">
    <!-- <li data-line="81" class="code-line" dir="auto">
    <p data-line="81" class="code-line" dir="auto"><code>__init__():</code>
    Accepts environment configuration (EnvConfig), RL parameters (RLParameters), computational device (device), and printing, and WandB integration.</p>
    </li> -->
    <!-- <li data-line="84" class="code-line" dir="auto">
    <p data-line="84" class="code-line" dir="auto"><code>_end_environments():</code>
    If the environment exists, shut down both the training and testing environments of a reinforcement learning setup.</p>
    </li> -->
    <li data-line="87" class="code-line" dir="auto">
    <p data-line="87" class="code-line" dir="auto"><code>train():</code>
    The main method of this class. To train the model based on the provided policy.</p>
    <ul data-line="90" class="code-line" dir="auto">
    <li data-line="90" class="code-line" dir="auto">setup(training=True): To prepare the training environment using this method</li>
    </ul>
    </li>
    <li data-line="92" class="code-line" dir="auto">
    <p data-line="92" class="code-line" dir="auto"><code>test():</code>
    To evaluate a reinforcement learning algorithm over specified of episodes(default: max_episode=100)</p>
    <ul data-line="94" class="code-line" dir="auto">
    <li data-line="94" class="code-line" dir="auto">setup(training=False): To prepare the test environment using this method</li>
    </ul>
    </li>
  </ul>
  <!-- <li data-line="96" class="code-line" dir="auto">
  <p data-line="96" class="code-line" dir="auto"><code>_train_step_logic():</code>
  The core method of the training process in a reinforcement learning environment.</p>
  <ul data-line="99" class="code-line" dir="auto">
  <li data-line="99" class="code-line" dir="auto"><code>helper.init_step():</code> initializes the training/testing step</li>
  <li data-line="100" class="code-line" dir="auto"><code>process_test_environmemt():</code> to handle interactions with an environment and for test interactions(<code>if 'training' == False</code>), the trajectories are not pushed to memory, which is a step reserved for training interaction.</li>
  </ul>
  </li> -->
  <!-- <li data-line="102" class="code-line" dir="auto">
  <p data-line="102" class="code-line" dir="auto"><code>_test_step_logic():</code>
  The logic for each step during the testing phase of a reinforcement learning algorithm.</p>
  </li> -->
  <!-- <li data-line="105" class="code-line" dir="auto">
  <p data-line="105" class="code-line" dir="auto"><code>train_on_policy():</code>
  To train a model using on-policy reinforcement learning algorithm.</p>
  <ul data-line="108" class="code-line" dir="auto">
  <li data-line="108" class="code-line" dir="auto"><code>process_train_environment():</code> Interacts with the training environment and collect data based on the current policy.</li>
  <li data-line="109" class="code-line" dir="auto"><code>should_update_strategy():</code> check if strategy should be updated.</li>
  <li data-line="110" class="code-line" dir="auto"><code>_update_strategy_from_samples():</code> to enhance the strategy used in the training of a reinforcement learning.</li>
  <li data-line="111" class="code-line" dir="auto"><code>reset_memory_and_train():</code> reset the memory buffer and then performs a seies of training steps. It iterates over a predefined number of on-policy iterations(<code>train_step</code>)</li>
  </ul>
  </li> -->
  <!-- <li data-line="113" class="code-line" dir="auto">
  <p data-line="113" class="code-line" dir="auto"><code>train_off_policy():</code>
  To train a model using off-policy reinforcement learning alrorithm.</p>
  <ul data-line="116" class="code-line" dir="auto">
  <li data-line="116" class="code-line" dir="auto"><code>process_train_environment():</code> Interacts with the training environment and collect data based on the current policy.</li>
  <li data-line="117" class="code-line" dir="auto"><code>should_update_strategy():</code> check if strategy should be updated.</li> -->
  <!-- <li data-line="118" class="code-line" dir="auto"><code>_update_strategy_from_samples():</code> to enhance the strategy used in the training of a reinforcement learning.</li> -->
  <!-- <li data-line="119" class="code-line" dir="auto"><code>train_step():</code> a single step of training in a reinforcement learning include Sample Trajectory, Model Training, TD Errors and Record Metrics</li>
  </ul>
  </li>
  <li data-line="122" class="code-line" dir="auto">
  <p data-line="122" class="code-line" dir="auto"><code>interact_environment():</code> This function progresses the environmentstate, retrieves the resulting trajectories, and the records these trajectories using the helper.record method.</p>
  </li>
  </ul> -->
  <br>
  <br>
  <div class="inline-container">
    <h2 data-line="91" class="class-rl-config" dir="auto" id="class-rl-config" tabindex="-1">Class RLParameters</h2>
    <a href="https://github.com/ccnets-team/rl-tune/blob/beta-causal-rl/utils/setting/rl_config.py" target="_blank" style="font-weight: 600; margin-left: 10px; margin-bottom:10px; font-size: 10px; font-size: smaller;">[source]</a>
  </div>
    <p data-line="70" class="code-line" dir="auto">The <code>RLParameters</code> class is designed as a structured approach to manage the settings for Reinforcement Learning (RL) configurations. This class encapsulates and manages multiple subsidiary parameter classes, each representing different components of an RL system. The detailed parameters for each component within the <code>RLParams</code> can be found in the <code>rl_config.py</code> </p>
    <h3 data-line="144" class="code-line" dir="auto" id="class-trainingparameters" tabindex="-1">Class TrainingParameters</h3>
    <ul data-line="145" class="code-line" dir="auto">
      <!-- <li data-line="145" class="code-line" dir="auto"><code>trainer_name</code>: Specifies the type of trainer algorithm. (default: <code>causal_rl</code>)</li> -->
      <!-- <li data-line="146" class="code-line" dir="auto"><code>trainer_variant</code>: Specifies a variant of <code>CausalRL</code>. (default: <code>classic</code>)</li> -->
      <!-- <li data-line="147" class="code-line" dir="auto"><code>use_on_policy</code>: Set whether the training is <code>on-policy</code> or <code>off-policy</code>. (default: <code>False</code>)</li> -->
      <li data-line="148" class="code-line" dir="auto"><code><em><p>batch_size</em></code>: Number of samples processed before model update. Larger batch size can lead to more stable but slower training. (<em>default: 64</em>)</p></li>
      <li data-line="149" class="code-line" dir="auto"><code><em><p>replay_ratio</em></code>: Ratio for how often past experiences are reused. (batch size / samples per step, <em>default: 1</em>)</p></li>
      <li data-line="150" class="code-line" dir="auto"><code><em><p>train_interval</em></code>: Frequency of thraining updates. (<em>default: 1</em>)</p></li>
      <li data-line="181" class="code-line" dir="auto"><code><em><p>max_steps</em></code>: Maximum number of steps for the exploration phase. (<em>default: 100000</em>)</p></li>
      <li data-line="185" class="code-line" dir="auto"><code><em><p>buffer_size</em></code>: Total size of the memory buffer, This determines the capacity for storing past experiences. (<em>default: 256000</em>)</p></li>
    </ul>
    <h3 data-line="153" class="code-line" dir="auto" id="class-algorithmparameters" tabindex="-1">Class AlgorithmParameters</h3>
    <ul data-line="154" class="code-line" dir="auto">
      <li data-line="155" class="code-line" dir="auto"><code><em><p>gpt_seq_length</em></code>: 
        The GPT sequence length is the maximum sequence length used during training and exploration. 
        In training, it is used to calculate the <em>Temporal Difference(TD) steps</em>, and in exploration, it is set as the maximum sequence length that can be input into GPT.
        A larger <code>gpt_seq_length</code> value allows for processing longer sequences, enabling the model to understand longer <em>state-action sequences</em> and learn long-term dependencies. 
        However, this can lead to increased memory usage and longer training times due to higher computational complexity. 
        On the other hand, a smaller <code>gpt_seq_length</code> value improves memory efficiency and can speed up training, 
        but it limits the model to only seeing short <em>state-action sequences</em>, making decisions based on short-term information. (<em>default: 16</em>)</p></li>
      <li data-line="156" class="code-line" dir="auto"><code><em><p>discount_factor</em></code>: To reduce the value of future rewards in the calculation of expected returns. (<em>default: 0.993</em>)</p></li>
      <li data-line="157" class="code-line" dir="auto"><code><em><p>advantage_lambda</em></code>: Weighting advantages in policy optimization. (<em>default: 0.95</em>)</p></li>
      <li data-line="158" class="code-line" dir="auto"><code><em><p>use_deterministic</em></code>: Determines whether to use deterministic actions during training/evaluation. (<em>default: False</em>)</p></li>
      <li data-line="159" class="code-line" dir="auto"><code><em><p>use_masked_exploration</em></code>: Enables sequence masking-based exploration, dynamically varying input sequence lengths by selective masking to promote internal strategy exploration without external noise. (<em>default: True</em>)</p></li>
    </ul>
    <h3 data-line="161" class="code-line" dir="auto" id="class-networkparameters" tabindex="-1">Class NetworkParameters</h3>
    <ul data-line="162" class="code-line" dir="auto">
      <li data-line="163" class="code-line" dir="auto"><code><em><p>num_layer</em></code>: Defines the number of layers in the neural network. (<em>default: 5</em>)</p></li>
      <li data-line="164" class="code-line" dir="auto"><code><em><p>d_model</em></code>: Specifies the dimensionality of the input and output of the model layers. (<em>default: 256</em>)</p></li>
      <li data-line="165" class="code-line" dir="auto"><code><em><p>dropout</em></code>: Sets the dropout rate to prevent overfitting. This is a regularization technique where randomly selected neurons are ignored during training. (<em>default: 0.02</em>)</p></li>
      <li data-line="164" class="code-line" dir="auto"><code><em><p>network_type</em></code>:  Determines the type of neural network architecture to use. This parameter allows the selection of different network architectures if needed. (<em>default: GPT</em>)</p></li>
      <li data-line="165" class="code-line" dir="auto"><code><em><p>critic_params</em></code>: Parameters for the critic network.</p></li>
      <li data-line="166" class="code-line" dir="auto"><code><em><p>actor_params</em></code>: Parameters for the actor network.</p></li>
      <li data-line="167" class="code-line" dir="auto"><code><em><p>rev_env_params</em></code>: Parameters for the rev-env network.</p></li>
    </ul>
    <h3 data-line="170" class="code-line" dir="auto" id="class-optimizationparameters" tabindex="-1">Class OptimizationParameters</h3>
    <ul data-line="171" class="code-line" dir="auto">
      <li data-line="171" class="code-line" dir="auto"><p><code><em>lr</em></code>: Learning rate for the optimiztion algorithm. (<em>default: 5e-5</em>)</p></li>
      <li data-line="172" class="code-line" dir="auto"><p><code><em>min_lr</em></code>: Minimum learning rate to which the lr will decay. (<em>default: 5e-6</em>)</p></li>
      <li data-line="173" class="code-line" dir="auto"><p><code><em>scheduler_type</em></code>: Specifies the type of learning rate scheduler. (<em>default: exponential</em>)</p></li>
      <li data-line="174" class="code-line" dir="auto"><p><code><em>tau</em></code>: How quickly the target network is updated with the main network's weights. (<em>default: 1e-1</em>)</p></li>
      <li data-line="175" class="code-line" dir="auto"><p><code><em>use_target_network</em></code>: Indicate whether to use target network or not. (<em>default: True</em>)</p></li>
      <li data-line="176" class="code-line" dir="auto"><p><code><em>clip_grad_range</em></code>: The range within which gradients are clipped. To prevent the issue of exploding gradients. (<em>default: None</em>)</p></li>
      <li data-line="177" class="code-line" dir="auto"><p><code><em>max_grad_norm</em></code>: A Parameter used in gradient clipping technique to prevent the "gradient explosion" issue by limiting the size of the gradients. It sets a threshold so that the L2 norm of all gradients does not exceed this value, helping to stabilize the training process. (<em>default: 1.0</em>)</p></li>
    </ul>
    <h3 data-line="189" class="code-line" dir="auto" id="class-normalizationparameters" tabindex="-1">Class NormalizationParameters</h3>
    <ul data-line="190" class="code-line" dir="auto">
      <li data-line="190" class="code-line" dir="auto">
      <p data-line="190" class="code-line" dir="auto"><code><em>state_normalizer</em></code>: Determines the method used for normalizing state valeus. (<em>default: running_mean_std</em>)</p>
      </li>
      <li data-line="191" class="code-line" dir="auto">
      <p data-line="191" class="code-line" dir="auto"><code><em>reward_normalizer</em></code>: Determines the method used for normalizing reward values. (<em>default: running_mean_std</em>)</p>
      </li>
      <li data-line="192" class="code-line" dir="auto">
      <p data-line="192" class="code-line" dir="auto"><code><em>sum_reward_normalizer</em></code>: Defines the method for normalizing reward values, using approaches like <em>'running_mean_std'</em> or <em>'None'</em>. (<em>default: running_abs_mean</em>)</p>
      </li>
      <li data-line="192" class="code-line" dir="auto">
      <p data-line="192" class="code-line" dir="auto"><code><em>advantage_normalizer</em></code>: Determines the method used for normalizing advantage values. (<em>default: running_abs_mean</em>)</p>
      </li>
    </ul>
  <br>
  <br>

  <div class="inline-container">
    <h2 data-line="91" class="class-causaltrainer" dir="auto" id="class-causaltrainer" tabindex="-1">Class CausalTrainer</h2>
    <a href="https://github.com/ccnets-team/rl-tune/blob/beta-causal-rl/training/causal_trainer.py" target="_blank" style="font-weight: 600; margin-left: 10px; margin-bottom:10px; font-size: 10px; font-size: smaller;">[source]</a>
  </div>
  <pre><code data-line="126" class="code-line language-python" dir="auto"><span class="hljs-keyword">class</span> <span class="hljs-title class_">CausalRL</span>(<span class="hljs-title class_ inherited__">BaseTrainer</span>):
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, rl_params, device</span>):
  </code></pre>
  <p data-line="131" class="code-line" dir="auto">This class is specialized trainer for reinforcement learning. It initailizes with RLparameters('rl_params'), and the computational device('device').
    And the class sets up three key networks: Critic, Actor, Reverse-Environment networks, each configured based on the provided RL parameters.</p>
    <h3 data-line="134" class="code-line" dir="auto" id="funtions" tabindex="-1">Method Overview</h3>
    <ul data-line="135" class="code-line" dir="auto">
      <li data-line="135" class="code-line" dir="auto">
        <p data-line="135" class="code-line" dir="auto"><code>get_action()</code> method is responsible for determining the action to be taken by an agent in a given state. When <code>training == True</code>, it uses Actor's <code>sample_action()</code> method else, it uses <code>select_action()</code> method.</p>
      </li>
      <li data-line="137" class="code-line" dir="auto">
        <p data-line="137" class="code-line" dir="auto"><code>train_model()</code> method is a core method of CausalRL approach. It involves a cooperative setup among three networks: Critic, Actor, Reverse-Environment, which learn from the environment's transitions. These method include computing various costs, errors, losses and these losses are used for backpropagation to adjust network parameters.</p>
      </li>
      <li data-line="139" class="code-line" dir="auto">
        <p data-line="139" class="code-line" dir="auto"><code>backwards()</code> method  conducts backpropagation for multiple neural networks in a reinforcement learning framework, each targeting a specific part of the causal relationship graph. It first disables, then selectively enables gradients for each network during the error backpropagation process to ensure targeted learning. Finally, it resets the networks to allow gradient updates, preparing them for future forward passes.</p>
      </li>
    </ul>
  <div data-line="196" class="code-line" dir="auto"></div>
  <br>
  <hr>
  <div align="left" style="padding-bottom: 10px;">
    Copyright Â© 2024 CCNets
  </div>
</div>
<div data-line="200" class="code-line" dir="auto"></div>

</ul>
<div data-line="213" class="code-line" dir="auto"></div>
<div data-line="215" class="code-line" dir="auto"></div>



      <script src="./style/js_funcs.js"></script>
      <script src="./style/script.js"></script>
    </body>
</html>
