<!DOCTYPE html>
<html>
    <head>
        <meta http-equiv="Content-type" content="text/html;charset=UTF-8">
        <title>Tutorial_Day1</title>
        <link rel="stylesheet" href="../style.css">

    </head>
    <body class="vscode-body vscode-light">
        <div data-line="0" class="code-line" dir="auto"></div>
<div data-line="2" class="code-line" dir="auto"></div>
<nav class="navbar">
<div  data-line="4" class="code-line" dir="auto"></div>
  <a class="header-link" href="../index.html">  
    <img src="https://github.com/ccnets-org/CausalRL.github.io/assets/95277008/6db8e929-637b-4d9c-9a61-20910e878d53" alt="CCNets Logo" class="logo">
    <!-- <h1 class="page-name">CausalRL Documentation</h1> -->
  </a>
<div data-line="7" class="code-line" dir="auto"></div>
<div class="social-links">
  <a href="https://ccnets.org/" target="_blank">
    <img src="https://github.com/ccnets-team/rl-tune/assets/95277008/42f10f53-0262-4203-9c6f-618e4841adb2" alt="Website" style="width: 20px; height: 20px;">
  </a>
  <a href="https://github.com/ccnets-team/rl-tune" target="_blank">
    <img src="https://github.com/ccnets-team/rl-tune/assets/95277008/5183f887-1263-408b-8c2f-9eeefafbce48" alt="GitHub" style="width: 20px; height: 20px;">
  </a>
  <a href="https://www.linkedin.com/company/ccnets/" target="_blank">
    <img src="https://github.com/ccnets-team/rl-tune/assets/95277008/fe1f76f2-a407-4f58-ad28-92a9ac9efa3f" alt="LinkedIn" style="width: 20px; height: 20px;">
  </a>
</div>
</nav>
<div data-line="9" class="code-line" dir="auto"></div>
<div class="sidebar">
  <ul>
  <li>
    <a href="../index.html">Introduction</a>
    <ul>
      <li><a href="../index.html#basic-usage">Basic Usage</a></li>
    </ul>
  </li>
  <li>
    <a href="../API.html">API</a>
    <ul>
      <li><a href="../API.html#class-rltune">RLTune</a></li>
      <li><a href="../API.html#class-causalrl">CausalRL</a></li>
      <li><a href="../API.html#class-rl-params">RLParams</a></li>
    </ul>
  </li>
  <li>
    <a href="../Env Wrapper.html">Env Wrapper</a>
    <ul>
      <li><a href="../Env Wrapper.html#gym-wrapper">Gym Wrapper</a></li>
      <li><a href="../Env Wrapper.html#unity-mlagent-wrapper">Unity MLAgent Wrapper</a></li>
    </ul>
  </li>
  <li>
    <a href="#">Tutorials</a>
    <ul>
      <li><a href="./tutorials/Tutorial_Day1.html">Day 1</a></li>
      <li><a href="#">Day 2</a></li>
    </ul>
  </li>
  <li>
    <a href="#">Development</a>
    <ul>
      <li><a href="https://github.com/ccnets-team/rl-tune">Github</a></li>
      <li><a href="https://wandb.ai/rl_tune/rl-tune-gym/?workspace">W&amp;B</a></li>
    </ul>
  </li>
</ul>
</div>
    <div class="main-content">
    <h1 data-line="0" class="code-line" dir="auto" id="causalrl-tutorial" tabindex="-1">CausalRL Tutorial</h1>
    <h2 data-line="2" class="code-line" dir="auto" id="day-1" tabindex="-1">Day 1</h2>
    <ul data-line="5" class="code-line" dir="auto">
    <li data-line="5" class="code-line" dir="auto">
    <p data-line="5" class="code-line" dir="auto">Envionment: <code>MLAgent 3DBallHard</code></p>
    </li>
    <li data-line="8" class="code-line" dir="auto">
    <p data-line="8" class="code-line" dir="auto">Models: <code>MLP</code>, <code>ResMLP</code></p>
    </li>
    </ul>
    <h2 data-line="11" class="code-line" dir="auto" id="introduction" tabindex="-1">Introduction</h2>
    <p data-line="13" class="code-line" dir="auto"><em>This tutorial is based on <strong>CCNets's causal-rl repository</strong> <code>beta branch</code>.</em></p>
    <p data-line="15" class="code-line" dir="auto">While CausalRL usually utilizes <strong>GPT models</strong> in each of its networks, this tutorial aims to simplify understanding by using relatively simple MLP models and the environment of Unity MLAgent. We will explore the training process and approach of CausalRL. Initially, CausalRL is composed of three networks: Critic, Actor, and Reverse Environment. This setup enhances the common Actor-Critic model used in many reinforcement learning algorithms by adding the unique Reverse Environment exclusive to CausalRL.</p>
    <p data-line="17" class="code-line" dir="auto">Typical reinforcement learning algorithms mainly consider transitions from the current to future states, predicting and optimizing future outcomes based on the agent's current state and actions. However, the Reverse Environment in CausalRL inversely undertakes this process, contemplating transitions from future to current states. It aims to understand <em>"which actions could have led to specific future states."</em> This approach allows CausalRL to delve deeper into understanding the causes,
    i.e., the causality of specific outcomes, and to trace back through the processes that led to them.</p>
    <p data-line="20" class="code-line" dir="auto">For more detailed information, please refer to the <a href="https://github.com/ccnets-team/causal-rl/blob/beta-causal-rl/README.md" data-href="https://github.com/ccnets-team/causal-rl/blob/beta-causal-rl/README.md">CausalRL Readme.md</a> or <a href="https://www.linkedin.com/company/ccnets/posts/?feedView=all" data-href="https://www.linkedin.com/company/ccnets/posts/?feedView=all">LinkedIn page of CCNets</a></p>
    <h2 data-line="22" class="code-line" dir="auto" id="dependency" tabindex="-1">Dependency</h2>
    <pre><code data-line="24" class="code-line language-bash" dir="auto">conda create -name crl python=3.9.18
    conda activate crl
    conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia
    pip install mlagents==0.30
    pip install protobuf==3.20
    pip install jupyter
    pip install transformers==4.34.1
    </code></pre>
    <h3 data-line="34" class="code-line" dir="auto" id="clone-the-repository" tabindex="-1">Clone the repository:</h3>
    <pre><code data-line="36" class="code-line language-bash" dir="auto">git <span class="hljs-built_in">clone</span> https://github.com/ccnets-team/causal-rl.git
    </code></pre>
    <h2 data-line="40" class="code-line" dir="auto" id="import-library" tabindex="-1">Import Library</h2>
    <ul data-line="42" class="code-line" dir="auto">
    <li data-line="42" class="code-line" dir="auto">
    <p data-line="42" class="code-line" dir="auto"><code>analyze_env()</code>: sets up the environment based on the <code>env_name</code> entered by the user and configures the <code>rl_params</code>.</p>
    </li>
    <li data-line="44" class="code-line" dir="auto">
    <p data-line="44" class="code-line" dir="auto"><code>set_seed()</code>: To control randomness by fixing the seed values for various libraries like <code>random</code>, <code>numpy</code>, and <code>torch</code>, ensuring that the experiments are stable and reproducible.</p>
    </li>
    </ul>
    <pre><code data-line="46" class="code-line language-python" dir="auto"><span class="hljs-keyword">from</span> __future__ <span class="hljs-keyword">import</span> print_function

    <span class="hljs-keyword">from</span> utils.setting.env_settings <span class="hljs-keyword">import</span> analyze_env
    <span class="hljs-keyword">from</span> utils.init <span class="hljs-keyword">import</span> set_seed
    <span class="hljs-keyword">import</span> torch

    set_seed()
    ngpu = <span class="hljs-number">2</span> 
    device = torch.device(<span class="hljs-string">"cuda:0"</span> <span class="hljs-keyword">if</span> (torch.cuda.is_available() <span class="hljs-keyword">and</span> ngpu &gt; <span class="hljs-number">0</span>) <span class="hljs-keyword">else</span> <span class="hljs-string">"cpu"</span>)
    </code></pre>
    <h2 data-line="58" class="code-line" dir="auto" id="setting-up-the-learning-environment" tabindex="-1">Setting Up the Learning Environment</h2>
    <p data-line="61" class="code-line" dir="auto">First, to use <strong>MLAgent's 3DBallHard</strong> in this tutorial, download the file from the link below and unzip it in the same location as your causal_rl directory.</p>
    <pre data-line="63" class="code-line" dir="auto"><code>Unity MLAgents(download link: https://drive.google.com/drive/folders/1TGSfw7IgfmVZslvmqIDLr5jAneQpsVbb?usp=sharing):
        locate the downloaded folder as below:
        your_projects/
            causal_rl/
            unity_environments/
    </code></pre>
    <div data-line="69" class="code-line" dir="auto"></div>
    <div style="text-align: center;">
        <img src="https://github.com/ccnets-org/CausalRL.github.io/assets/95277008/b2ed57f1-9ed9-4b88-ad8f-40adce1a23e4" alt="3DBallHard" style="width: 500px;">
    </div>
    <p data-line="74" class="code-line" dir="auto">In the <strong>3DBallHard environment</strong>, the agent (a cube) uses 45 states to balance a ball on its head, taking 2 continuous actions. Specifically, one action value corresponds to rotation along the X-axis, and the other to rotation along the Z-axis. At each step, the agent decides and takes actions based on the given state information. Here, CausalRL uses the Reverse Environment to trace back the actions taken by the agent and infer causality. For instance, it retraces the process by asking, "What direction and rotation actions taken by the agent in the past might have caused the ball to fall off its head?"</p>
    <p data-line="76" class="code-line" dir="auto">For more detailed information about the environment, please refer to <a href="https://unity-technologies.github.io/ml-agents/Learning-Environment-Examples/#3dball-3d-balance-ball" data-href="https://unity-technologies.github.io/ml-agents/Learning-Environment-Examples/#3dball-3d-balance-ball">MLAgent Docs</a>.</p>
    <pre><code data-line="78" class="code-line language-python" dir="auto">env_config, rl_params = analyze_env(env_name = <span class="hljs-string">"3DBallHard"</span>)
    </code></pre>
    <h2 data-line="82" class="code-line" dir="auto" id="models" tabindex="-1">Models</h2>
    <p data-line="84" class="code-line" dir="auto"><strong>Model Description:</strong></p>
    <ul data-line="86" class="code-line" dir="auto">
    <li data-line="86" class="code-line" dir="auto">
    <p data-line="86" class="code-line" dir="auto"><code>MLP</code> (<em>Multi-Layer Perceptron</em>): The <code>MLP</code> is a standard multi-layer perceptron, a neural network with multiple layers. Each layer comprises linear transformations, ReLU activation functions, and dropout, facilitating learning of complex data patterns. This model is suited for relatively simple problems and used for basic pattern recognition and decision-making.</p>
    </li>
    <li data-line="89" class="code-line" dir="auto">
    <p data-line="89" class="code-line" dir="auto"><code>ResMLP</code> (<em>Residual Multi-Layer Perceptron</em>): The <code>ResMLP</code> enhances the <code>MLP</code> with residual connections. These connections aid in deeper learning by mitigating the gradient vanishing problem. It is advantageous for solving complex problems and enabling deeper learning.</p>
    </li>
    </ul>
    <h2 data-line="92" class="code-line" dir="auto" id="networks" tabindex="-1">Networks</h2>
    <div data-line="94" class="code-line" dir="auto"></div>
    <div style="text-align: center;">
        <img src="https://github.com/ccnets-org/CausalRL.github.io/assets/95277008/9fe09609-e296-4578-9f4a-0248bb98517b" alt="CausalRL" style="width: 700px;">
    </div>
    <p data-line="98" class="code-line" dir="auto"><strong>Network Description:</strong></p>
    <ul data-line="100" class="code-line" dir="auto">
    <li data-line="100" class="code-line" dir="auto">
    <p data-line="100" class="code-line" dir="auto"><code>Critic</code>: Takes the <em>Sampled State</em> as input and generates an <em>Explanation Vector</em>, an interpretation of that <em>Sampled State</em> by the <code>Critic</code>. This <em>Explanation Vector</em> is passed to the <code>Actor</code> and <code>Rev-Env</code>, used for evaluating the quality and long-term outcomes of the <em>Sampled State</em>.</p>
    </li>
    <li data-line="102" class="code-line" dir="auto">
    <p data-line="102" class="code-line" dir="auto"><code>Actor</code>: Receives the <code>Critic</code>'s <em>Explanation Vector</em> and the <em>Sampled State</em> as input, predicting the agent's <em>Next Action</em>. It focuses on Action prediction based on the <em>Current State</em>.</p>
    </li>
    <li data-line="105" class="code-line" dir="auto">
    <p data-line="105" class="code-line" dir="auto"><code>Rev-Env</code>: Inputs include the <code>Critic</code>'s <em>Explanation Vector</em>, the <code>Actor</code>'s <em>Predicted Action</em>, the actual action taken (<em>Sampled Action</em>), and the <em>Next state</em>. It infers and generates <em>Recurred State</em> and <em>Reversed State</em> from these inputs.</p>
    </li>
    </ul>
    <p data-line="108" class="code-line" dir="auto">In summary, the <code>Actor</code> in this tutorial performs a relatively simple role, predicting action based on the <em>Sampled State</em>, hence requiring a less complex model. The <code>Critic</code>, on the other hand, needs to understand both the <em>Sampled State</em> and potential future states, calling for a more complex model. The <code>Rev-Env</code> is tasked with using the <em>Sampled Action</em>, <em>Next State</em>, the <code>Actor</code>'s <em>predicted action</em>, and explanations of <em>Sampled State</em> to infer <em>Sampled State</em>, a process that is significantly more complex than forward modeling. Therefore, among the three networks, <code>Rev-Env</code> demands the most sophisticated and complex model is required. Threfore, in this tutorial, we plan to assign the more complex ResMLP network for both the <code>Critic</code> and <code>Rev-Env</code> due to their intricate roles, and the simpler <code>MLP</code> will be utilized for the <code>Actor</code>.</p>
    <pre><code data-line="110" class="code-line language-python" dir="auto"><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn

    <span class="hljs-keyword">class</span> <span class="hljs-title class_">ResBlock</span>(nn.Module):
        <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, hidden_dim, dropout</span>):
            <span class="hljs-built_in">super</span>(ResBlock, self).__init__()
            self.linear1 = nn.Linear(hidden_dim, hidden_dim)
            self.linear2 = nn.Linear(hidden_dim, hidden_dim)
            self.relu = nn.ReLU()
            self.dropout = nn.Dropout(dropout)
            
        <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):
            residual = x
            out = self.linear1(x)
            out = self.relu(out)
            out = self.linear2(out)
            out += residual
            out = self.relu(out)
            out = self.dropout(out)  
            <span class="hljs-keyword">return</span> out

    <span class="hljs-keyword">class</span> <span class="hljs-title class_">ResMLP</span>(nn.Module):
        <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, num_layer, d_model, dropout</span>):
            hidden_dim, num_blocks = d_model, num_layer
            <span class="hljs-built_in">super</span>(ResMLP, self).__init__()
            self.layers = nn.Sequential(
                *(ResBlock(hidden_dim, dropout=dropout) <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_blocks))
            )
        
        <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x, mask = <span class="hljs-literal">None</span></span>):
            out = self.layers(x)
            <span class="hljs-keyword">return</span> out
        
    <span class="hljs-keyword">class</span> <span class="hljs-title class_">MLP</span>(nn.Module):
        <span class="hljs-keyword">def</span> <span class="hljs-title function_">create_deep_modules</span>(<span class="hljs-params">self, layers_size, dropout</span>):
            deep_modules = []
            <span class="hljs-keyword">for</span> in_size, out_size <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(layers_size[:-<span class="hljs-number">1</span>], layers_size[<span class="hljs-number">1</span>:]):
                deep_modules.append(nn.Linear(in_size, out_size))
                deep_modules.append(nn.ReLU())
                deep_modules.append(nn.Dropout(dropout))
            <span class="hljs-keyword">return</span> nn.Sequential(*deep_modules)

        <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, num_layer, d_model, dropout</span>):
            <span class="hljs-built_in">super</span>(MLP, self).__init__()   
            self.deep = self.create_deep_modules([d_model] + [<span class="hljs-built_in">int</span>(d_model) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_layer)], dropout)
                    
        <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x, mask = <span class="hljs-literal">None</span></span>):
            x = self.deep(x)
            <span class="hljs-keyword">return</span> x
    </code></pre>
    <h2 data-line="161" class="code-line" dir="auto" id="training" tabindex="-1">Training</h2>
    <p data-line="163" class="code-line" dir="auto">We will assign MLP and ResMLP to the networks as previously described and begin training.</p>
    <p data-line="165" class="code-line" dir="auto">If you want to visually monitor the training process, you can enable this by setting use_graphics = True.</p>
    <p data-line="167" class="code-line" dir="auto">For those familiar with reinforcement learning, you can optimize training for the environment by modifying the default setting parameters. You can do this by adjusting the training parameters in the <code>rl_config.py</code> to aim for higher scores.</p>
    <pre><code data-line="169" class="code-line language-python" dir="auto"><span class="hljs-keyword">from</span> causal_rl <span class="hljs-keyword">import</span> CausalRL

    rl_params.network.actor_network = MLP
    rl_params.network.critic_network = ResMLP
    rl_params.network.rev_env_network = ResMLP

    <span class="hljs-keyword">with</span> CausalRL(env_config, rl_params, device, use_graphics = <span class="hljs-literal">True</span>, use_print = <span class="hljs-literal">True</span>) <span class="hljs-keyword">as</span> causal_rl:
        causal_rl.train(resume_training = <span class="hljs-literal">False</span>)
    </code></pre>
    <div data-line="181" class="code-line" dir="auto"></div>
    <div style="text-align: center;">
        <img src="https://github.com/ccnets-org/CausalRL.github.io/assets/95277008/e70603ea-cd62-44de-a608-13c9b8885399" alt="Train_result" style="width: 1000px;">
    </div>
    </div>
    <script src="script.js"></script>
    </body>
</html>
